{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation \n",
    "This notebook tests and assesses the performance of different retrieval methods and reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You try to use a model that was created with version 3.0.1, however, your version is 2.7.0. This might cause unexpected behavior or errors. In that case, try to update to the latest version.\n",
      "\n",
      "\n",
      "\n",
      "/mnt/nvme/home/durech/camille/rag/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:210: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/mnt/nvme/home/durech/camille/rag/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n",
      "Some weights of the model checkpoint at dunzhang/stella_en_400M_v5 were not used when initializing NewModel: ['new.pooler.dense.bias', 'new.pooler.dense.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/mnt/nvme/home/durech/camille/rag/lib/python3.10/site-packages/sentence_transformers/models/Dense.py:77: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(input_path, \"pytorch_model.bin\"), map_location=torch.device(\"cpu\"))\n",
      "llama_model_loader: loaded meta data with 26 key-value pairs and 291 tensors from /tmp/llama_index/models/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = Meta-Llama-3-8B-Instruct\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128001\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  22:                      quantize.imatrix.file str              = /models/Meta-Llama-3-8B-Instruct-GGUF...\n",
      "llama_model_loader: - kv  23:                   quantize.imatrix.dataset str              = /training_data/groups_merged.txt\n",
      "llama_model_loader: - kv  24:             quantize.imatrix.entries_count i32              = 224\n",
      "llama_model_loader: - kv  25:              quantize.imatrix.chunks_count i32              = 88\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.8000 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = Meta-Llama-3-8B-Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128001 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4685.30 MiB\n",
      "........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3904\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   488.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  488.00 MiB, K (f16):  244.00 MiB, V (f16):  244.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   283.63 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
      "Model metadata: {'quantize.imatrix.entries_count': '224', 'quantize.imatrix.dataset': '/training_data/groups_merged.txt', 'quantize.imatrix.chunks_count': '88', 'quantize.imatrix.file': '/models/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct.imatrix', 'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.eos_token_id': '128001', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '8192', 'general.name': 'Meta-Llama-3-8B-Instruct', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.file_type': '15', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: llama-3\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import faiss\n",
    "import chromadb\n",
    "\n",
    "sys.path.append('../')\n",
    "from utils import retriever_evaluation, display_results_retriever\n",
    "\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    StorageContext\n",
    ")\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.postprocessor.colbert_rerank import ColbertRerank\n",
    "from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "#import data, queries and embedding model\n",
    "qa_dataset_path = \"../data/icrc_qa_dataset_semantic2_2_2048.pkl\"\n",
    "nodes_path = \"../data/nodes_icrc_semantic2_2_2048.pkl\"\n",
    "model_url = \"https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf\"\n",
    "embed_model = \"dunzhang/stella_en_400M_v5\"\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name =embed_model, trust_remote_code=True)\n",
    "nodes = pickle.load(open(nodes_path,'rb'))\n",
    "qa_dataset = pickle.load(open(qa_dataset_path,'rb'))\n",
    "llm_llama3 = LlamaCPP(\n",
    "    model_url=model_url,\n",
    "    model_path=None,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=512,\n",
    "    context_window=3900,\n",
    "    generate_kwargs={},\n",
    "    model_kwargs={\"n_gpu_layers\": -1},\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 2048/2048 [01:04<00:00, 31.83it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [01:07<00:00, 30.47it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:40<00:00, 50.76it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:37<00:00, 54.15it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:41<00:00, 48.94it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:42<00:00, 48.24it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:44<00:00, 46.09it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:42<00:00, 48.25it/s]\n",
      "Generating embeddings: 100%|██████████| 2048/2048 [00:49<00:00, 41.25it/s]\n",
      "Generating embeddings: 100%|██████████| 1988/1988 [00:50<00:00, 39.20it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retriever Name</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Base Retriever</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.551333</td>\n",
       "      <td>0.271599</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Retriever Name  hit_rate       mrr      ndcg\n",
       "0  Base Retriever     0.658  0.551333  0.271599"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding model as a retriever\n",
    "index = VectorStoreIndex(nodes, embed_model=embed_model, show_progress=True) #compute indexes\n",
    "base_retriever = index.as_retriever(similarity_top_k=3) #build retriever\n",
    "base_retriever_evaluator = retriever_evaluation(base_retriever, metrics=[\"hit_rate\",\"mrr\",\"ndcg\"]) #set evaluator\n",
    "base_eval_results2 = await base_retriever_evaluator.aevaluate_dataset(qa_dataset) #evaluate\n",
    "display_results_retriever(\"Base Retriever\", base_eval_results2, metrics=[\"hit_rate\",\"mrr\",\"ndcg\"]) #display results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Re-Ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Retriever Name</th>\n",
       "      <th>hit_rate</th>\n",
       "      <th>mrr</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>ap</th>\n",
       "      <th>ndcg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Base and bge Retriever</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.652667</td>\n",
       "      <td>0.239333</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.652667</td>\n",
       "      <td>0.31417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Retriever Name  hit_rate       mrr  precision  recall        ap  \\\n",
       "0  Base and bge Retriever     0.718  0.652667   0.239333   0.718  0.652667   \n",
       "\n",
       "      ndcg  \n",
       "0  0.31417  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using a reranker\n",
    "base_retriever = index.as_retriever(similarity_top_k=10)\n",
    "bge_reranker = FlagEmbeddingReranker(\n",
    "    top_n=3,\n",
    "    model=\"BAAI/bge-reranker-large\", # \"Alibaba-NLP/gte-Qwen2-7B-instruct\"\n",
    "    use_fp16=False\n",
    ")\n",
    "\n",
    "base_bge_retriever_evaluator = retriever_evaluation(base_retriever, node_postprocessor=[bge_reranker], metrics =[\"hit_rate\", \"mrr\", \"precision\", \"recall\", \"ap\", \"ndcg\"])\n",
    "\n",
    "base_bge_eval_results =  await base_bge_retriever_evaluator.aevaluate_dataset(qa_dataset)\n",
    "display_results_retriever(\"Base and bge Retriever\", base_bge_eval_results, metrics =[\"hit_rate\", \"mrr\", \"precision\", \"recall\", \"ap\", \"ndcg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_retriever = index.as_retriever(similarity_top_k=10)\n",
    "colbert_reranker = ColbertRerank(\n",
    "    top_n=3,\n",
    "    model=\"colbert-ir/colbertv2.0\",\n",
    "    tokenizer=\"colbert-ir/colbertv2.0\",\n",
    "    keep_retrieval_score=True,\n",
    ")\n",
    "base_colbert_retriever_evaluator = retriever_evaluation(base_retriever, node_postprocessor=[colbert_reranker], metrics =[\"hit_rate\", \"mrr\", \"precision\", \"recall\", \"ap\", \"ndcg\"])\n",
    "\n",
    "base_colbert_eval_results =  await base_colbert_retriever_evaluator.aevaluate_dataset(qa_dataset)\n",
    "display_results_retriever(\"Base and colbert Retriever\", base_colbert_eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_retriever = index.as_retriever(similarity_top_k=10)\n",
    "bge_reranker = FlagEmbeddingReranker(\n",
    "    top_n=3,\n",
    "    model=\"BAAI/bge-reranker-base\", # \"Alibaba-NLP/gte-Qwen2-7B-instruct\"\n",
    "    use_fp16=False\n",
    ")\n",
    "\n",
    "base_bge_retriever_evaluator = retriever_evaluation(base_retriever, node_postprocessor=[bge_reranker], metrics =[\"hit_rate\", \"mrr\", \"precision\", \"recall\", \"ap\", \"ndcg\"])\n",
    "\n",
    "base_bge_eval_results =  await base_bge_retriever_evaluator.aevaluate_dataset(qa_dataset)\n",
    "display_results_retriever(\"Base and bge Retriever\", base_bge_eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_retriever = index.as_retriever(similarity_top_k=10)\n",
    "postprocessor = SentenceTransformerRerank(\n",
    "model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\",\n",
    " top_n=5\n",
    ")\n",
    "\n",
    "base_ce_retriever_evaluator = retriever_evaluation(base_retriever, node_postprocessor=[postprocessor], metrics =[\"hit_rate\", \"mrr\", \"precision\", \"recall\", \"ap\", \"ndcg\"])\n",
    "\n",
    "base_ce_eval_results =  await base_ce_retriever_evaluator.aevaluate_dataset(qa_dataset)\n",
    "display_results_retriever(\"Base and bge Retriever\", base_ce_eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25retriever = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=3)\n",
    "BM25_retriever_evaluator = retriever_evaluation(BM25retriever)\n",
    "BM25_eval_results =  await BM25_retriever_evaluator.aevaluate_dataset(qa_dataset)\n",
    "display_results_retriever(\"BM25 Retriever\", BM25_eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25retriever = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=10)\n",
    "colbert_reranker = ColbertRerank(\n",
    "    top_n=3,\n",
    "    model=\"colbert-ir/colbertv2.0\",\n",
    "    tokenizer=\"colbert-ir/colbertv2.0\",\n",
    "    keep_retrieval_score=True,\n",
    ")\n",
    "\n",
    "BM25_colbert_retriever_evaluator = retriever_evaluation(BM25retriever, node_postprocessor=[colbert_reranker], metrics =[\"hit_rate\", \"mrr\", \"ndcg\"])\n",
    "\n",
    "BM25_colbert_eval_results =  await BM25_colbert_retriever_evaluator.aevaluate_dataset(qa_dataset)\n",
    "display_results_retriever(\"BM25 and colbert Retriever\", BM25_colbert_eval_results, [\"hit_rate\", \"mrr\", \"ndcg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25retriever = BM25Retriever.from_defaults(nodes=nodes, similarity_top_k=10)\n",
    "bge_reranker = FlagEmbeddingReranker(\n",
    "    top_n=3,\n",
    "    model=\"BAAI/bge-reranker-base\",\n",
    "    use_fp16=False\n",
    ")\n",
    "\n",
    "BM25_bge_retriever_evaluator = retriever_evaluation(BM25retriever, node_postprocessor=[bge_reranker], metrics =[\"hit_rate\", \"mrr\", \"precision\", \"recall\", \"ap\", \"ndcg\"])\n",
    "\n",
    "BM25_bge_eval_results =  await BM25_bge_retriever_evaluator.aevaluate_dataset(qa_dataset)\n",
    "display_results_retriever(\"BM25 and bge Retriever\", BM25_bge_eval_results, [\"hit_rate\", \"mrr\", \"ap\", \"ndcg\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 1024\n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "\n",
    "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex(nodes, storage_context=storage_context, embed_model = embed_model\n",
    ")\n",
    "\n",
    "base_retriever = index.as_retriever(similarity_top_k=3)\n",
    "base_bge_retriever_evaluator = retriever_evaluation(base_retriever, metrics =[\"hit_rate\", \"mrr\"])\n",
    "base_bge_eval_results =  await base_bge_retriever_evaluator.aevaluate_dataset(qa_dataset)\n",
    "display_results_retriever(\"Base and bge Retriever\", base_bge_eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create client and a new collection\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "chroma_collection = chroma_client.create_collection(\"quickstart\")\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex(\n",
    "    nodes, storage_context=storage_context, embed_model=embed_model\n",
    ")\n",
    "\n",
    "from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker\n",
    "base_retriever = index.as_retriever(similarity_top_k=3)\n",
    "\n",
    "base_bge_retriever_evaluator = retriever_evaluation(base_retriever, metrics =[\"hit_rate\", \"mrr\"])\n",
    "\n",
    "base_bge_eval_results =  await base_bge_retriever_evaluator.aevaluate_dataset(qa_dataset)\n",
    "display_results_retriever(\"Base and bge Retriever\", base_bge_eval_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.elasticsearch import ElasticsearchStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "vector_store = ElasticsearchStore(\n",
    "    es_cloud_id=\"\",\n",
    "    es_api_key=\"\",  # see Elasticsearch Vector Store for more authentication options\n",
    "    index_name=\"test\",\n",
    "    embed_model = embed_model,\n",
    "    model = llm_llama3\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex(nodes, storage_context=storage_context, show_progress = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_retriever = index.as_retriever(similarity_top_k=3)\n",
    "base_bge_retriever_evaluator = retriever_evaluation(base_retriever, metrics =[\"hit_rate\", \"mrr\"])\n",
    "base_bge_eval_results =  await base_bge_retriever_evaluator.aevaluate_dataset(qa_dataset)\n",
    "display_results_retriever(\"Base and bge Retriever\", base_bge_eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LiGHT_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
